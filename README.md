# Mini MRM Credit Validation Pack

A baseline **PD (Probability of Default)** model plus a compact **Model Risk Management (MRM) validation package**: data quality controls, reproducible dataset build (SQL ETL), model training, performance testing (AUC/KS), calibration (Brier + calibration curve), and stability diagnostics (PSI), with a short validation memo.

## What this project demonstrates (MRM lens)
- **Data Quality Controls (DQC):** missingness, duplicates, variable profiling  
- **Lineage / Reproducibility:** raw → processed modeling table via SQL + versioned outputs  
- **Baseline PD model:** Logistic Regression with categorical encoding  
- **Validation evidence:** discrimination, calibration, stability + figures  
- **Documentation:** validator-style memo with limitations and next steps  

## Repo structure
- `src/` — ingestion, QC, table build, training, validation  
- `sql/` — reproducible ETL (`01_build_model_table.sql`)  
- `data/raw/` — raw snapshot generated by ingest (often not committed)  
- `data/processed/` — analysis-ready `model_table.csv`  
- `outputs/models/` — saved pipeline artifact (`.joblib`)  
- `outputs/metrics/` — QC tables + validation metrics (JSON/CSV)  
- `reports/figures/` — ROC, calibration curve, score distributions  
- `reports/validation_memo.md` — validation write-up  

## Quickstart (end-to-end)
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

python -m src.ingest
python -m src.qc
python -m src.build_table
python -m src.train
python -m src.validate
Key outputs (where to look)
QC pack:
outputs/metrics/qc_missingness.csv
outputs/metrics/qc_profile_numeric.csv
outputs/metrics/qc_profile_categorical.csv
Processed modeling table: data/processed/model_table.csv
Trained model: outputs/models/logit_pd_pipeline.joblib
Validation metrics: outputs/metrics/validation_metrics.json
Stability (PSI): outputs/metrics/psi_score.json
Figures:
reports/figures/roc_curve_test.png
reports/figures/calibration_curve_test.png
reports/figures/score_hist_test.png
Validation memo: reports/validation_memo.md
Results snapshot (test split)
From outputs/metrics/validation_metrics.json:
AUC (test): 0.818
KS (test): 0.590
Brier (test): 0.147
PSI score (train vs test): 0.018
Configuration
Project settings live in config.yaml (e.g., random_seed, test_size, missingness threshold, psi_bins).
Limitations / model risk notes
Public dataset (“credit-g”) and proxy outcome (good/bad credit), not a production default definition.
Random holdout split (not time-based), so PSI here is not a true drift-over-time test.
No reject inference, no fairness testing, and no business policy constraints modeled.
Next steps (high ROI)
Add time-based evaluation (if time is available) + monitoring plan (PSI + performance drift over time).
Add explainability (top drivers) and simple model governance metadata (versioning, approvals, change log).
